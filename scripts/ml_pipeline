import numpy as np
import pandas as pd
from sklearn.metrics import mean_squared_error,r2_score
from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, VotingRegressor, StackingRegressor
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LinearRegression, Lasso, Ridge, SGDRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor
from sklearn.neural_network import MLPRegressor
import pickle
from data_pipeline import X_train_scaled, X_test_scaled, y_train, y_test, X_train


def main():
    """
    Performs model training.
    Output: The saved model
    """

    def model_pipeline (models, k):
        mean_score_list=[]
        model_list=[]
        for model in models:
            scores = cross_val_score(model,X_train_scaled,y_train,scoring="r2",cv=k)
            mean_score = np.mean(scores)
            mean_score_list.append(mean_score)
            model_list.append(model)
            print(f'Mean cross-validation r2 for {model} is {mean_score}')
        best_model=model_list[np.argmax(mean_score_list)]
        return print(f'The best model is {best_model}')


    #Checking baseline models for cross validation = 3
    k=3

    models = [LinearRegression(),
            Lasso(),
            Ridge(),
            SGDRegressor(),
            KNeighborsRegressor(),
            SVR(),
            DecisionTreeRegressor(),
            AdaBoostRegressor(),
            GradientBoostingRegressor(),
            RandomForestRegressor(),
            XGBRegressor()
            ]

    print(model_pipeline (models, k))

    #Random forest model hyperparameter tuning:

    rf = RandomForestRegressor(max_depth=20, min_samples_split=5, min_samples_leaf=1, max_samples=1.0, max_features=0.5, n_estimators=500)
    rf.fit(X_train_scaled, y_train)
    rf_y_pred_test = rf.predict(X_test_scaled)
    r2_rf = r2_score(y_test, rf_y_pred_test)
    rmse_rf = mean_squared_error(y_test, rf_y_pred_test, squared=False)
    print(round(r2_rf,2))
    print(round(rmse_rf,2))

    def feature_importance(model,feat_names):
        importances = model.feature_importances_
        indices = np.argsort(importances)[::-1]
        ranked_feats = []
        for i in range(len(indices)):
            feat_name = feat_names[indices[i]]
            ranked_feats.append(feat_name)
        ranking = pd.DataFrame()
        ranking['Feat Index'] = indices
        ranking['Feature'] = ranked_feats
        ranking['Importance'] = np.sort(importances)[::-1]
        return ranking.iloc[:5,:]

    features = list(X_train.columns)
    feature_importance(rf, features)

   
if __name__ == '__main__':
    main()
import numpy as np
import pandas as pd
from sklearn.metrics import mean_squared_error,r2_score
from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, VotingRegressor, StackingRegressor
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LinearRegression, Lasso, Ridge, SGDRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor
from sklearn.neural_network import MLPRegressor
import pickle
from data_pipeline import X_train_scaled, X_test_scaled, y_train, y_test, X_train


def main():
    """
    Performs model training.
    Output: The saved model
    """

    def model_pipeline (models, k):
        mean_score_list=[]
        model_list=[]
        for model in models:
            scores = cross_val_score(model,X_train_scaled,y_train,scoring="r2",cv=k)
            mean_score = np.mean(scores)
            mean_score_list.append(mean_score)
            model_list.append(model)
            print(f'Mean cross-validation r2 for {model} is {mean_score}')
        best_model=model_list[np.argmax(mean_score_list)]
        return print(f'The best model is {best_model}')


    #Checking baseline models for cross validation = 3
    k=3

    models = [LinearRegression(),
            Lasso(),
            Ridge(),
            SGDRegressor(),
            KNeighborsRegressor(),
            SVR(),
            DecisionTreeRegressor(),
            AdaBoostRegressor(),
            GradientBoostingRegressor(),
            RandomForestRegressor(),
            XGBRegressor()
            ]

    print(model_pipeline (models, k))

    #Random forest model hyperparameter tuning:

    rf = RandomForestRegressor(max_depth=20, min_samples_split=5, min_samples_leaf=1, max_samples=1.0, max_features=0.5, n_estimators=500)
    rf.fit(X_train_scaled, y_train)
    rf_y_pred_test = rf.predict(X_test_scaled)
    r2_rf = r2_score(y_test, rf_y_pred_test)
    rmse_rf = mean_squared_error(y_test, rf_y_pred_test, squared=False)
    print(round(r2_rf,2))
    print(round(rmse_rf,2))

    def feature_importance(model,feat_names):
        importances = model.feature_importances_
        indices = np.argsort(importances)[::-1]
        ranked_feats = []
        for i in range(len(indices)):
            feat_name = feat_names[indices[i]]
            ranked_feats.append(feat_name)
        ranking = pd.DataFrame()
        ranking['Feat Index'] = indices
        ranking['Feature'] = ranked_feats
        ranking['Importance'] = np.sort(importances)[::-1]
        return ranking.iloc[:5,:]

    features = list(X_train.columns)
    feature_importance(rf, features)

    #XGBoost  model hyperparameter tuning:

    xgb = XGBRegressor(max_depth=10, min_child_weight=2, learning_rate=0.05, max_leaves=2, max_bin=2, n_estimators=500)
    xgb.fit(X_train_scaled, y_train)
    xgb_y_pred_test = xgb.predict(X_test_scaled)
    r2_xgb = r2_score(y_test, xgb_y_pred_test)
    rmse_xgb = mean_squared_error(y_test, xgb_y_pred_test, squared=False)
    print(round(r2_xgb,2))
    print(round(rmse_xgb,2))

    #MLP  model hyperparameter tuning:

    mlp = MLPRegressor(hidden_layer_sizes=(250,), learning_rate_init=0.01, alpha=0.001)
    mlp.fit(X_train_scaled, y_train)
    mlp_y_pred_test = mlp.predict(X_test_scaled)
    r2_mlp = r2_score(y_test, mlp_y_pred_test)
    rmse_mlp = mean_squared_error(y_test, mlp_y_pred_test, squared=False)
    print(round(r2_mlp,2))
    print(round(rmse_mlp,2))

    #Other good performing models:

    svr = SVR(C=40)
    gb = GradientBoostingRegressor(n_estimators=200)

    #Ensemble model 1
    ensemble_model = VotingRegressor(estimators=[('rf',rf),
                                                ('gb',gb),  
                                                ('svr',svr), 
                                                ('xgb',xgb), 
                                                ('mlp',mlp)]
                                    )

    ensemble_model.fit(X_train_scaled,y_train)

    test_preds1 = ensemble_model.predict(X_test_scaled)
    r2_ensemble_model = r2_score(y_test, test_preds1)
    rmse_ensemble_model = mean_squared_error(y_test, test_preds1, squared=False)
    print(round(r2_ensemble_model,2))
    print(round(rmse_ensemble_model,2))

    #Ensemble model 2

    ensemble_model2 = VotingRegressor(estimators=[('rf',rf),
                                                ('gb',gb),  
                                                ('svr',svr), 
                                                ('xgb',xgb), 
                                                ('mlp',mlp)],
                                    weights = [2,1,1,1,2])

    ensemble_model2.fit(X_train_scaled,y_train)

    test_preds2 = ensemble_model2.predict(X_test_scaled)
    r2_ensemble_model2 = r2_score(y_test, test_preds2)
    rmse_ensemble_model2 = mean_squared_error(y_test, test_preds2, squared=False)
    print(round(r2_ensemble_model2,2))
    print(round(rmse_ensemble_model2,2))

    #Stacking ensemble

    stacking_model = StackingRegressor(estimators=[('rf',rf),
                                                ('gb',gb),  
                                                ('svr',svr), 
                                                ('xgb',xgb), 
                                                ('mlp',mlp)],
                                    final_estimator=mlp,
                                    cv=3)

    stacking_model.fit(X_train_scaled,y_train)

    test_preds3 = stacking_model.predict(X_test_scaled)
    r2_stacking_model = r2_score(y_test, test_preds3)
    rmse_stacking_model = mean_squared_error(y_test, test_preds3, squared=False)
    print(round(r2_stacking_model,2))
    print(round(rmse_stacking_model,2))

    # Saving the model
    with open('model.pkl','wb') as f:
        pickle.dump(stacking_model,f)

    #print(final_model.get_params())

if __name__ == '__main__':
    main()